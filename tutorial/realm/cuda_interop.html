<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Realm CUDA Interop &#8211; Legion Programming System</title>
<meta name="description" content="">

<meta name="keywords" content="">


<!-- mermaid Graph -->


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Realm CUDA Interop">
<meta property="og:description" content="Home page for the Legion parallel programming system">
<meta property="og:url" content="/tutorial/realm/cuda_interop.html">
<meta property="og:site_name" content="Legion Programming System">





<link rel="canonical" href="/tutorial/realm/cuda_interop.html">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Legion Programming System Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.min.css">

<!--[if (lt IE 9) & (!IEMobile)]>
<link rel="stylesheet" href="/assets/css/ie.min.css">
<![endif]-->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<link rel="icon" sizes="16x16" href="/images/favicon/favicon-16x16.png">
<link rel="icon" sizes="32x32" href="/images/favicon/favicon-32x32.png">
<link rel="icon" sizes="48x48" href="/images/favicon/favicon-48x48.png">
<link rel="icon" sizes="96x96" href="/images/favicon/favicon-96x96.png">
<link rel="icon" sizes="144x144" href="/images/favicon/favicon-144x144.png">
<link rel="icon" sizes="192x192" href="/images/favicon/favicon-192x192.png">
<link rel="apple-touch-icon" sizes="57x57" href="/images/favicon/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/images/favicon/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/images/favicon/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/images/favicon/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/images/favicon/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/images/favicon/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/images/favicon/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/images/favicon/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-icon-180x180.png">
<link rel="apple-touch-icon-precomposed" sizes="192x192" href="/images/favicon/apple-icon-precomposed.png">
<meta name="msapplication-config" content="/images/favicon/browserconfig.xml" />
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<link rel="manifest" href="/images/favicon/manifest.json">

</head>

<body class="page" itemscope itemtype="http://schema.org/WebPage">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="/">Legion Programming System</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" itemscope itemtype="http://schema.org/SiteNavigationElement">
		    <ul>
		        
				<li><a href="/overview/" >Overview</a></li>
		        
				<li><a href="/starting/" >Getting Started</a></li>
		        
				<li><a href="/tutorial/" >Tutorials</a></li>
		        
				<li><a href="/events/" >Events</a></li>
		        
				<li><a href="/documentation/" >Documentation</a></li>
		        
				<li><a href="/publications/" >Publications</a></li>
		        
				<li><a href="/resources/" >Resources</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->




<div id="main" role="main"  itemprop="mainContentOfPage">
  <div class="article-author-side">
    <a href="https://www.stanford.edu/"><img src="/images/logos/stanford.png" class="bio-photo" alt="Stanford University logo"></a>
<a href="https://www6.slac.stanford.edu/"><img src="/images/logos/slac.jpg" class="bio-photo" alt="SLAC National Accelerator Laboratory logo"></a>
<a href="https://www.lanl.gov/"><img src="/images/logos/los-alamos.png" class="bio-photo" alt="Los Alamos National Laboratory logo"></a>
<a href="https://www.nvidia.com/"><img src="/images/logos/nvidia.png" class="bio-photo" alt="NVIDIA logo"></a>
<a href="https://www.rdworldonline.com/rd-100-award-winners-announced-in-analytical-test-it-electrical-categories/"><img src="/images/logos/rd100.png" class="bio-photo" alt="Winner of the R&D 100 Award"></a>

<h3>Legion</h3>
<p>A Data-Centric Parallel Programming System</p>





<a href="http://github.com/StanfordLegion/legion" class="author-social" target="_blank"><i class="icon-github"></i> Github</a>



  </div>
  <article itemscope itemtype="http://schema.org/CreativeWork">
    <h1 itemprop="name">Realm CUDA Interop</h1>
    <div class="article-wrap" itemprop="text">
      <h2 id="introduction">Introduction</h2>
<p>In this tutorial, we look at how to interoperate with CUDA in order to utilize
the Realm task-based programming model to take advantage of the raw computing power
of the GPU.</p>

<p>This tutorial will review several features and best practices for working with
Realm and CUDA, listed below:</p>

<ul>
  <li><a href="#enabling-cuda-in-realm">Enabling CUDA in Realm</a></li>
  <li><a href="#registering-gpu-tasks">Registering GPU Tasks</a></li>
  <li><a href="#allocating-memory-through-realm">Allocating Memory Through Realm</a>
    <ul>
      <li><a href="#cuda-arrays">CUDA Arrays</a></li>
    </ul>
  </li>
  <li><a href="#data-movement">Data Movement</a></li>
  <li><a href="#best-practices">Best Practices</a></li>
</ul>

<h2 id="enabling-cuda-in-realm">Enabling CUDA in Realm</h2>

<p>To enable CUDA in Realm, we need to ensure we have an appropriate CUDA Toolkit available to compile with.  Follow your system’s documentation or the official <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" title="CUDA Programming Guide">CUDA Documentation</a> on how to do this.</p>

<p>If using CMAKE to build Realm, all that is required to enable CUDA is to add <code class="language-plaintext highlighter-rouge">-DLegion_USE_CUDA=ON -DCUDA_TOOLKIT_ROOT_DIR=&lt;path_to_cuda&gt;</code> to our normal cmake configure line.  For example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>cmake <span class="nt">-DLegion_USE_CUDA</span><span class="o">=</span>ON <span class="nt">-DCUDA_TOOLKIT_ROOT_DIR</span><span class="o">=</span>/usr/local/cuda ..
</code></pre></div></div>

<p>Realm provides various low-level command line arguments to configure how GPUs are set up prior to application use.  One of these is <code class="language-plaintext highlighter-rouge">-ll:gpus N</code>, which will tell Realm the number of GPUs that Realm can use. -&gt;
An example of a command line argument is -ll:gpus N, which specifies the number of GPUs that Realm can utilize.  When combined with CUDA_VISIBLE_DEVICES or CUDA_DEVICE_ORDER, or some of Realm’s other low level command line arguments, we can define exactly what GPU is associated with the specific instance of the application.</p>

<p>Other notable low-level command line arguments are the following:</p>

<table>
  <thead>
    <tr>
      <th>Argument</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-ll:gpus N</code></td>
      <td>Associates the first N gpus to this application</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:skipgpus N</code></td>
      <td>Number of gpus to skip between each chosen GPU</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-ll:gpu_ids x,y,z</code></td>
      <td>Specifies the CUDA device ids of the GPUs to associate with the application</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-ll:pin</code></td>
      <td>Register local processor’s memory to pin for DMA use</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:hostreg N</code></td>
      <td>Specify the amount of memory on local processors to pin for DMA use (default is 1GiB)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:callbacks</code></td>
      <td>Allow Realm managed fences to use cuStreamAddCallback instead of polling on cudaEvents</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:nohijack</code></td>
      <td>Suppress the runtime warning about the CUDA Hijack not active</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:skipbusy</code></td>
      <td>Skip any gpus that do not initialize</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:minavailmem N</code></td>
      <td>Skip any gpus that do not have at minimum N MiB of memory</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:legacysync</code></td>
      <td>Track task CUDA progress with an event on the default stream (does not capture NON_BLOCKING streams)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:contextsync</code></td>
      <td>Force Realm to call cuCtxSynchronize after a <code class="language-plaintext highlighter-rouge">TOC_PROC</code> task completes</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:maxctxsync N</code></td>
      <td>Maximum number of outstanding background context synchronization threads</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:lmemresize</code></td>
      <td>Set the CU_CTX_LMEM_RESIZE_TO_MAX flag on the context</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:mtdma  </code></td>
      <td>Enable multi-threaded DMA</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:ipc</code></td>
      <td>Use legacy <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#interprocess-communication" title="CUDA Legacy Interprocess Communication">CUDA IPC</a> for interprocess communication</td>
    </tr>
  </tbody>
</table>

<h2 id="registering-gpu-tasks">Registering GPU Tasks</h2>

<p>Looking at <code class="language-plaintext highlighter-rouge">main()</code> in the tutorial, we see most of the standard Realm boilerplate code.  The main difference is that when we want to do something with CUDA, we want to target the <code class="language-plaintext highlighter-rouge">TOC_PROC</code> (also known as the <em>throughput optimized compute processor</em>) rather than the <code class="language-plaintext highlighter-rouge">LOC_PROC</code> kind (also known as the <em>latency optimized compute processor</em>).  It is because when a task is run on a <code class="language-plaintext highlighter-rouge">TOC_LOC</code> processor, Realm will properly set up the thread state to target the associated device.  In addition, any CUDA work launched or enqueued in a stream will automatically be synchronized in the background after the thread has returned from the task.  We will get back to why this is important later.</p>

<p>To target a <code class="language-plaintext highlighter-rouge">TOC_PROC</code> for CUDA, we simply change what kind of processors a task id can target when we register it:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Processor</span><span class="o">::</span><span class="n">register_task_by_kind</span><span class="p">(</span><span class="n">Processor</span><span class="o">::</span><span class="n">TOC_PROC</span><span class="p">,</span>
                                 <span class="nb">false</span> <span class="cm">/*!global*/</span><span class="p">,</span>
                                 <span class="n">MAIN_TASK</span><span class="p">,</span>
                                 <span class="n">CodeDescriptor</span><span class="p">(</span><span class="n">main_task</span><span class="p">),</span>
                                 <span class="n">ProfilingRequestSet</span><span class="p">(),</span>
                                 <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="n">wait</span><span class="p">();</span>
</code></pre></div></div>

<p>Afterwards, we just need to find a <code class="language-plaintext highlighter-rouge">TOC_PROC</code> to spawn the main task onto:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Processor</span> <span class="n">p</span> <span class="o">=</span> <span class="n">Machine</span><span class="o">::</span><span class="n">ProcessorQuery</span><span class="p">(</span><span class="n">Machine</span><span class="o">::</span><span class="n">get_machine</span><span class="p">())</span>
                  <span class="p">.</span><span class="n">only_kind</span><span class="p">(</span><span class="n">Processor</span><span class="o">::</span><span class="n">TOC_PROC</span><span class="p">)</span>
                  <span class="p">.</span><span class="n">first</span><span class="p">();</span>
</code></pre></div></div>

<p>And just like any other processor, launch the registered GPU task on it!</p>

<h2 id="allocating-memory-through-realm">Allocating Memory Through Realm</h2>

<p>For this tutorial, we are looking at doing something relatively simple: allocate a linear 2D array and a <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#surface-object-api" title="CUDA Surface Object API">cudaSurfaceObject</a>, fill each with some data and copy the linear array to the cudaSurfaceObject.  In order to fully leverage Realm’s data movement and other asynchronous features, we will create RegionInstances for each of these memories.  This gives us the same interface to work with that we have seen in previous tutorials.</p>

<p>Allocating linear GPU memory is fairly close to how it is done for standard CPU.  Below are highlights from <code class="language-plaintext highlighter-rouge">main_task()</code>:</p>

<p>1) Define the memory layout (remember Realm’s bounds are inclusive, whereas CUDA’s are not, so we subtract one from the width and height here).</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="o">&gt;</span> <span class="n">field_sizes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="n">Rect</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span> <span class="n">bounds</span><span class="p">(</span><span class="n">Point</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">Point</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">height</span> <span class="o">-</span> <span class="mi">1</span><span class="p">));</span>
</code></pre></div></div>

<p>2) Find the specific memory you want to allocate that suits your needs.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Memory</span> <span class="n">gpu_mem</span> <span class="o">=</span> <span class="n">Machine</span><span class="o">::</span><span class="n">MemoryQuery</span><span class="p">(</span><span class="n">Machine</span><span class="o">::</span><span class="n">get_machine</span><span class="p">())</span>
                     <span class="p">.</span><span class="n">has_capacity</span><span class="p">(</span><span class="n">bounds</span><span class="p">.</span><span class="n">volume</span><span class="p">()</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">))</span>
                     <span class="p">.</span><span class="n">best_affinity_to</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
                     <span class="p">.</span><span class="n">first</span><span class="p">();</span>
</code></pre></div></div>

<p>3) Create a Realm::RegionInstance, just like in previous tutorials.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">RegionInstance</span><span class="o">::</span><span class="n">create_instance</span><span class="p">(</span><span class="n">linear_instance</span><span class="p">,</span> <span class="n">gpu_mem</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">field_sizes</span><span class="p">,</span>
                                <span class="cm">/*SOA*/</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ProfilingRequestSet</span><span class="p">());</span>
</code></pre></div></div>

<p>Looking at the MemoryQuery, just like ProcessorQuery, there are different memory kinds specific to GPUs that can be leveraged depending on various use cases.  By default, this tutorial just picks the best one that has enough room for our work, but an application may want specific features of other memory kinds.  These memory kinds are described as follows:</p>

<table>
  <thead>
    <tr>
      <th>Memory Kind</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">GPU_FB_MEM</code></td>
      <td>Pre-allocated cudaMalloc() memory managed by Realm</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">GPU_DYNAMIC_MEM</code></td>
      <td>Maps to cudaMalloc() for each instance creation</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">GPU_MANAGED_MEM</code></td>
      <td>Pre-allocated cudaMallocManaged() memory managed by Realm</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Z_COPY_MEM</code></td>
      <td>Pre-allocated cudaMallocHost() memory managed by Realm</td>
    </tr>
  </tbody>
</table>

<p>For more information on the features of each of these kinds, consult the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" title="CUDA Programming Guide">CUDA Documentation</a>.  Each of these has command line arguments for controlling the size of these memories:</p>

<table>
  <thead>
    <tr>
      <th>Argument</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-ll:fsize N</code></td>
      <td>Specify the pre-allocated size of GPU_FB_MEM</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-ll:zsize N</code></td>
      <td>Specify the pre-allocated size of Z_COPY_MEM</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-ll:msize N</code></td>
      <td>Specify the pre-allocated size of GPU_MANAGED_MEM</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:dynfb</code></td>
      <td>Enable the <code class="language-plaintext highlighter-rouge">GPU_DYNAMIC_MEM</code> memory kind</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cuda:dynfb_max N</code></td>
      <td>Cap the size of <code class="language-plaintext highlighter-rouge">GPU_DYNAMIC_MEM</code> to the specified amount (default is the GPU’s framebuffer size)</td>
    </tr>
  </tbody>
</table>

<p>For now, we will just have Realm pick the one that matches our requirements using <code class="language-plaintext highlighter-rouge">best_affinity_to()</code>.</p>

<h3 id="cuda-arrays">CUDA Arrays</h3>

<p>Now that the linear memory is allocated, time to allocate the <code class="language-plaintext highlighter-rouge">cudaSurfaceObject_t</code>.  <code class="language-plaintext highlighter-rouge">cudaSurfaceObject_t</code> is great to use if processing elements requires neighborhood lookups (e.g., image convolutions) because their layout is not linear. However a <a href="https://en.wikipedia.org/wiki/Z-order_curve" title="Z-Order Curves">Z-order curve</a>, also known as “block linear”, improves cache utilization for such access patterns.</p>

<p>While a little more complicated, allocating CUDA surface objects can be essentially broken down into the same basic steps as linear memory with one exception; instead of querying for what memory to use, we will allocate the memory directly with CUDA APIs and use the <code class="language-plaintext highlighter-rouge">ExternalCudaArrayResource</code> class to register the memory with Realm to use.  This means we have to manage the memory lifetime ourselves, but we can still leverage all that Realm provides after we complete the registration:</p>

<p>1) Allocate the cuda array and bind a cudaSurfaceObject_t to be used later.  If this code is unfamiliar to you, check out the <a href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/0_Introduction/simpleSurfaceWrite" title="simpleSurfaceWrite CUDA Sample">simpleSurfaceWrite</a> CUDA sample for more information.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cudaArray_t</span> <span class="n">array</span><span class="p">;</span>
<span class="n">cudaSurfaceObject_t</span> <span class="n">surface_obj</span><span class="p">;</span>
<span class="n">cudaExtent</span> <span class="n">extent</span><span class="p">;</span>
<span class="n">cudaChannelFormatDesc</span> <span class="n">fmt</span> <span class="o">=</span>
    <span class="n">cudaCreateChannelDesc</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cudaChannelFormatKindFloat</span><span class="p">);</span>
<span class="n">cudaResourceDesc</span> <span class="n">resource_descriptor</span> <span class="o">=</span> <span class="p">{};</span>

<span class="n">extent</span><span class="p">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">width</span><span class="p">;</span>
<span class="n">extent</span><span class="p">.</span><span class="n">height</span> <span class="o">=</span> <span class="n">height</span><span class="p">;</span>
<span class="n">extent</span><span class="p">.</span><span class="n">depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">cudaMalloc3DArray</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">fmt</span><span class="p">,</span> <span class="n">extent</span><span class="p">,</span> <span class="n">cudaArraySurfaceLoadStore</span><span class="p">);</span>

<span class="n">resource_descriptor</span><span class="p">.</span><span class="n">resType</span> <span class="o">=</span> <span class="n">cudaResourceTypeArray</span><span class="p">;</span>
<span class="n">resource_descriptor</span><span class="p">.</span><span class="n">res</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="p">;</span>
<span class="n">cudaCreateSurfaceObject</span><span class="p">(</span><span class="o">&amp;</span><span class="n">surface_obj</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">resource_descriptor</span><span class="p">);</span>
</code></pre></div></div>

<p>2) Describe the layout of the memory to Realm.  We have to describe this as a non-affine layout and utilize the <code class="language-plaintext highlighter-rouge">CudaArrayLayoutPiece</code> class in order to tell Realm this is not ordinarily linear memory and needs to be treated as special.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">InstanceLayout</span><span class="o">&lt;</span><span class="mi">2</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">layout</span><span class="p">;</span>
    <span class="n">InstanceLayoutGeneric</span><span class="o">::</span><span class="n">FieldLayout</span> <span class="o">&amp;</span><span class="n">field_layout</span> <span class="o">=</span> <span class="n">layout</span><span class="p">.</span><span class="n">fields</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>

    <span class="n">layout</span><span class="p">.</span><span class="n">space</span> <span class="o">=</span> <span class="n">IndexSpace</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">bounds</span><span class="p">);</span>
    <span class="n">field_layout</span><span class="p">.</span><span class="n">list_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">field_layout</span><span class="p">.</span><span class="n">rel_offset</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">field_layout</span><span class="p">.</span><span class="n">size_in_bytes</span> <span class="o">=</span> <span class="n">field_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
    <span class="n">layout</span><span class="p">.</span><span class="n">piece_lists</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="c1">// Specify a non-affine layout by specifying a CudaArrayLayoutPiece that spans the</span>
    <span class="c1">// entire cuda array</span>
    <span class="n">CudaArrayLayoutPiece</span><span class="o">&lt;</span><span class="mi">2</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="o">*</span><span class="n">layout_piece</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CudaArrayLayoutPiece</span><span class="o">&lt;</span><span class="mi">2</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span><span class="p">;</span>
    <span class="n">layout_piece</span><span class="o">-&gt;</span><span class="n">bounds</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">;</span>
    <span class="n">layout</span><span class="p">.</span><span class="n">piece_lists</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">pieces</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">layout_piece</span><span class="p">);</span>

    <span class="n">ExternalCudaArrayResource</span> <span class="nf">cuda_array_external_resource</span><span class="p">(</span><span class="n">gpu_idx</span><span class="p">,</span> <span class="n">array</span><span class="p">);</span>
</code></pre></div></div>

<p>3) Create a Realm::RegionInstance with the given ExternalCudaArrayResource, this time using create_external_instance instead.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">RegionInstance</span><span class="o">::</span><span class="n">create_external_instance</span><span class="p">(</span>
    <span class="n">array_instance</span><span class="p">,</span> <span class="n">cuda_array_external_resource</span><span class="p">.</span><span class="n">suggested_memory</span><span class="p">(),</span>
    <span class="n">layout</span><span class="p">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">cuda_array_external_resource</span><span class="p">,</span> <span class="n">ProfilingRequestSet</span><span class="p">());</span>
</code></pre></div></div>

<h2 id="data-movement">Data Movement</h2>

<p>Now that we have allocated memory for the data, let us fill it in.  The logic on how to do this with Realm is exactly the same as in previous tutorials:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Fill the linear array with ones.</span>
<span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_fill</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">1.0</span><span class="n">f</span><span class="p">);</span>
<span class="n">dsts</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_field</span><span class="p">(</span><span class="n">linear_instance</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">field_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">fill_done_event</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">srcs</span><span class="p">,</span> <span class="n">dsts</span><span class="p">,</span> <span class="n">ProfilingRequestSet</span><span class="p">(),</span> <span class="n">fill_done_event</span><span class="p">);</span>
</code></pre></div></div>

<p>As of this writing, Realm does not currently support non-affine fill operations, so we will reuse <code class="language-plaintext highlighter-rouge">linear_instance</code> in order to first fill it, then copy it to the array properly:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Copy the linear array to the cuda array, filling it with zeros.</span>
<span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_field</span><span class="p">(</span><span class="n">linear_instance</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">field_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">dsts</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_field</span><span class="p">(</span><span class="n">array_instance</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">field_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">fill_done_event</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">srcs</span><span class="p">,</span> <span class="n">dsts</span><span class="p">,</span> <span class="n">ProfilingRequestSet</span><span class="p">(),</span> <span class="n">fill_done_event</span><span class="p">);</span>
</code></pre></div></div>

<p>While we could use CUDA APIs to do these operations, we would first need to synchronize on the creation of the instances directly.  With Realm, we can describe the events that are needed to coordinate and execute all of these operations asynchronously.</p>

<p>So far, we have created Realm managed GPU-visible linear memory, registered a CUDA Array with Realm, and initialized these with appropriate values.  We now have everything we need to launch work on the GPU with CUDA.  For this tutorial, we will be using Realm’s AffineAccessor from the device, as it can contain both the device address and the strided layout of the memory.</p>

<p>As an aside, sometimes you need to pass the GPU address to a library or device code directly, which you can get from the linear_accessor.  Additionally, for N-dimensional instances like this, we will need to make sure to handle larger strides of memory chosen by Realm for performance reasons in order to index it properly:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="o">*</span><span class="n">linear_ptr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">linear_accessor</span><span class="p">[</span><span class="n">Point</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)];</span>
<span class="kt">size_t</span> <span class="n">pitch_in_elements</span> <span class="o">=</span> <span class="n">linear_accessor</span><span class="p">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">linear_accessor</span><span class="p">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</code></pre></div></div>

<p>The device code for processing is fairly simple, but the key highlights are the fact that we can use some Realm structures like <code class="language-plaintext highlighter-rouge">Rect&lt;N,T&gt;</code>, <code class="language-plaintext highlighter-rouge">AffineAccessor&lt;T,N&gt;</code>, and <code class="language-plaintext highlighter-rouge">Point&lt;N,T&gt;</code>.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">copyKernel</span><span class="p">(</span><span class="n">Rect</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">AffineAccessor</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span> <span class="n">linear_accessor</span><span class="p">,</span>
                           <span class="n">cudaSurfaceObject_t</span> <span class="n">surface</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">size_t</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">size_t</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="c1">// Have each thread copy the value from the linear array to the surface</span>
  <span class="k">for</span><span class="p">(;</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">bounds</span><span class="p">.</span><span class="n">volume</span><span class="p">();</span> <span class="n">tid</span> <span class="o">+=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">size_t</span> <span class="n">y</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">/</span> <span class="p">(</span><span class="n">bounds</span><span class="p">.</span><span class="n">hi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
    <span class="kt">size_t</span> <span class="n">x</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">-</span> <span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="n">bounds</span><span class="p">.</span><span class="n">hi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
    <span class="kt">float</span> <span class="n">value</span> <span class="o">=</span> <span class="n">linear_accessor</span><span class="p">[</span><span class="n">Point</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)];</span>
    <span class="n">surf2Dwrite</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">surface</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">y</span><span class="p">,</span> <span class="n">cudaBoundaryModeTrap</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Once the data is processed, we can read it back by creating some host visible memory to copy to. When creating the instance for reading, we can choose any memory that our <code class="language-plaintext highlighter-rouge">check_processor</code> has an affinity to (and can therefore access).  In the tutorial, we let Realm pick the memory to use; however, we can specify that we want memory that is also accessible to the GPU in order to optimize the copy operation.  Without GPU access to the memory, the copy would have to be chunked up and done in stages, which can add extra overhead for large copies.  However, it may be beneficial for the case when such communication is infrequent or when pinned memory resources are limited.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cpu_mem</span> <span class="o">=</span> <span class="n">Machine</span><span class="o">::</span><span class="n">MemoryQuery</span><span class="p">(</span><span class="n">Machine</span><span class="o">::</span><span class="n">get_machine</span><span class="p">())</span>
              <span class="p">.</span><span class="n">has_capacity</span><span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">))</span>
              <span class="p">.</span><span class="n">has_affinity_to</span><span class="p">(</span><span class="n">check_processor</span><span class="p">)</span>
              <span class="p">.</span><span class="n">has_affinity_to</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
              <span class="p">.</span><span class="n">first</span><span class="p">();</span>
</code></pre></div></div>

<p>Then we can perform a copy transfer just like we did earlier to this new CPU visible instance and launch our checking task to ensure the results are correct.  We use a <code class="language-plaintext highlighter-rouge">LOC_PROC</code> processor for this checking task as this is a CPU bound task, and there is no GPU management in this task.  It allows us to free up the <code class="language-plaintext highlighter-rouge">TOC_PROC</code> processor for more GPU work if we want to.</p>

<p>And that’s all there is to it!</p>

<h2 id="best-practices">Best Practices</h2>

<p>To recap some best practices mentioned above:</p>

<ul>
  <li>
    <p><strong>Try not to call <code class="language-plaintext highlighter-rouge">cuda*Synchronize</code> APIs like <code class="language-plaintext highlighter-rouge">cudaDeviceSynchronize()</code></strong>.  These calls block the thread and prevent Realm from cooperatively scheduling other for that processor.  Instead, try to queue work in streams, coordinate parallel streams with CUDA events and allow the task to complete without synchronizing these streams.  Realm will ensure all the streams will synchronize prior to the task being considered complete by the application.</p>
  </li>
  <li>
    <p><strong>Ensure both tasks and the device work launched can make forward progress</strong>.  Just like how tasks in Realm need to be scheduled via Realm event’s wait() and the like to ensure the forward progress of the application, CUDA device code also needs this guarantee.  This guarantee can be harder to enforce with device code that needs external communication, like a flag on a local CPU, RDMA with a NIC, or even between blocks launched on the device.  Please see the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" title="CUDA Programming Guide">CUDA Programming Guide</a> for more information, specifically parts dealing with device-side synchronization like <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cooperative-groups" title="CUDA Cooperative Groups">CUDA Cooperative Groups</a>.</p>
  </li>
  <li>
    <p><strong>Use <code class="language-plaintext highlighter-rouge">TOC_PROC</code> type processors when manipulating GPU state</strong>.  These are tied to a specific GPU state, and the current device context is already set up when the task runs.</p>
  </li>
  <li>
    <p><strong>Consider using <code class="language-plaintext highlighter-rouge">LOC_PROC</code> processors for heavy CPU work</strong>: <code class="language-plaintext highlighter-rouge">TOC_PROC</code> processor kinds are meant to manipulate GPU state, e.g., launching work, querying device information, etc.  These processors can be limited in CPU processing threads, or can NUMA bound to a specific NUMA node, or do heavier weight operations to ensure a consistent GPU state.  This makes <code class="language-plaintext highlighter-rouge">TOC_PROC</code> processors ill-suited for heavily CPU-bound tasks, which is why we launch a <code class="language-plaintext highlighter-rouge">LOC_PROC</code> task for our checking task in the tutorial code.</p>
  </li>
  <li>
    <p><strong>Pick memories and processors that have affinities to each other</strong>.  It can achieve the best data movement performance.</p>
  </li>
  <li>
    <p>If an application does not care about some of the specific features a particular memory kind provides, try to <strong>allow Realm to choose the best memory</strong> to use by utilizing MemoryQuery and has_affinity_to, best_affinity_to, and has_capacity.  This allows an application to be flexible to the current state of the system as well as what the system may provide without application modifications.</p>
  </li>
  <li>
    <p><strong>Take into account the application’s access patterns to memory</strong>.  For example, large allocations that infrequently need to be updated between the CPU and GPU do not need to have an affinity to both the CPU and GPU.  As such memory is normally allocated in pinned system memory, this can bottleneck the GPU’s access to the memory due to much lower bandwidth; however, it can also consume system memory resources such that other parts of the application (or even the entire system) can under perform.  Instead, if an allocation is primarily accessed from the CPU and is infrequently updated, it may be best just to use CPU local memory.  On the other hand, if there is a lot of GPU/CPU communication happening, using memory that has an affinity to both may justify the cost.  See section 13.1 in the <a href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Best_Practices_Guide.pdf" title="CUDA Best Practices Guide">CUDA Best Practices Guide</a> for more.</p>
  </li>
  <li>
    <p><strong>Register pre-allocated CUDA memories with Realm with RegionInstances</strong>.  This allows the application to leverage Realm’s asynchronous programming model, data movement, and task partitioning mechanisms to its fullest</p>
  </li>
</ul>

<p>More best practices in regard to CUDA can be found in the <a href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Best_Practices_Guide.pdf" title="CUDA Best Practices Guide">CUDA Best Practices Guide</a> and the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" title="CUDA Programming Guide">CUDA Programming Guide</a>.</p>


    </div><!-- /.article-wrap -->
  </article>
</div><!-- /#index -->

<div class="footer-wrap">
  <footer>
    <span>&copy; 2025 Legion. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl = 
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-20524102-3']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

          

</body>
</html>
